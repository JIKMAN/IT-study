# 하둡의 탄생

왜 하둡이 필요할까? 왜 여러 개의 디스크를 가진 데이터베이스를 이용해 대규모 분석을 수행할 수 없을까?

'탐색 시간은 전송 속도보다 발전이 더디다'.. 탐색은 데이터를 읽거나 쓸 때 디스크의 헤더를 디스크의 특정 위치로 이동시키는 조작이다. 전송속도는 디스크의 대역폭과 관련되어 있다.

![image-20211129212452448](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20211129212452448.png)







우리는 데이터 홍수의시대에 살고 있으며, 하둡은 비정형 데이터를 포함한 빅데이터를 다루기 위
한 가장 적절한 플랫폼

탄생 - 더그 커팅이 lucene이라는 오픈소스를 아파치로 공개하면서 등장, 검색엔진의 인덱싱 라이브러리를 기반으로 너치Nutch 라는 프로젝트를 탄생 -> nutch 진행중 하둡 프로젝트 탄생, 그 이후로 하둡 관련 에코시스템 등장

너치 프로젝트의 문제점 : 웹 크롤링한 데이터를 저장 후 루신 라이브러리로 색인을 해야하는데, 수십 수 조개의 데이터 굉장히 큰 데이터를 분산 병렬 처리 해야되는 어려움 -> 2003 년 구글에서 GFS 논문을 발표, 2004년 제프 딘이 맵 리듀스라는 구글의 알고리즘을 발표 -> 구글의 파일 시스템 아키텍쳐를 기반으로 너치 파일 시스템이라는 NDFS 개발하기 시작 -> Yahoo에서 하둡을 전격적으로 지원하면서 아파치 탑프로젝트



하둡은 자바로 개발되어 있음. 파이썬 C++ 등 지원

스크립트 언어 지원 - Pig, Hive, Spark

Hbase : NoSQL 분산 데이터베이스 

주키퍼 : coordinating

Mahout : 

Avro : 경량화

sqoop : RDBMS와 하둡 연결

Hcatalog

Oozie, Airflow : workflow를 스케쥴링 하는 스케쥴러



하둡으로 인해 더 이상 많은 데이터를 저장하기 위해 큰 비용이 들지 않음

통신 비용이 점점 싸지고, 예전에 데이터로 남기지 않던 데이터를 다 남기고있는 시대가 되었고 iot등 센서 데이터들을 저장, 데이터 저장에 필요한 인프라 구축의 기반이 적어짐





# HDFS

구글파일 시스템의 기본 아키텍처와 비슷

마스터/슬레이브 구조, 슬레이브 서버는 N대로 확장해 나갈 수 있음

마스터에 부하가 가지않는 상황을 만드는게 중요

마스터와 데이터를 주고받는 경우는 없도록 설계되어있음

마스터의 안전성을 가장 보장할 수 있는 형태로 구성되어 있음



구글 플랫폼 철학

1. 한대의 고가 장비보다여러 대의 저가 장비가 낫다

   스케일 아웃

2. 데이터는 분산저장한다

   perallel (cpu를 병렬로 처리를 하는것을 의미, 데이터를 공용 스토리지에 저장해놓고 cpu 코어 수나 메모리를 여러 개로 늘려 가면서 처리 하는 것) vs distribute (데이터를 분산하고 분산된 데이터를 처리)

3. 시스템(H/W)은언제든 죽을 수 있다 (Smart S/W)

4. 시스템 확장이쉬워야 한다

   데이터 양이 늘어났을 때, 아키텍처 전체를 바꾸고 장비 스펙을 올리고 개발을 새로할 필요 없이 장비만 더 추가해서 클러스터 노드수만 늘리면 된다.

하둡은 이런 구글플랫폼의 철학을 그대로 가져와 수천 대 이상의 범용 리눅스 기반 서버들을 하나의 클러스터로 사용하는 형태로 클러스터를 구성  (온프레미스 또는 클라우드로)



하둡 특성

•수천대 이상의 리눅스기반 범용 서버들을 하나의 클러스터로 사용
•마스터-슬레이브 구조
•파일은 블록(block) 단위로 저장
•블록 데이터의 복제본유지로 인한 신뢰성 보장(기본 3개의 복제본)
•높은 내고장성(Fault-Tolerance)
•데이터 처리의 지역성보장



하둡은 한대 두대는 의미가 없다.



하둡 1.0

![image-20211129214313221](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20211129214313221.png)

슬레이브에는 데이터를 저장 관리하는 data node, 어플리케이션 업무를 수행하게 되는 task tracker

 분산 파일 시스템에 저장되어 있는 데이터에 대한 잡을 어플리케이션 연산을 처리하기 위해서 어플리케이션 관리하는 마스터 데몬은 Job tracker라는 마스터데몬

hdfs 분산 파일 시스템을 관리하는 마스터 데몬 :  네임 노드



hdfs 블록이 큰 이유는 하둡 분산 파일 시스템에 네임노드 혹은 데이터 노드 혹은 테스트 트레커 들이 블록을 탐색할 때 파일의 정보를 최대한 빨리 찾기 위해 메타 정보의 줄이기 위한 용도, 그래야 나중에 큰 데이터들을 빨리 처리할 수 있기 때문

블록이 크면 블록의 시작점 탐색에 걸리는 시간이 굉장히 줄어든다.

네트워크 전송을 하는데 어디 파일이 어디있는지 찾는데 리소스를 쓰는게 아니고 네트웍을 통해 데이터를 전송 시키는데 많은 시간을 할당이 가능하다.



하둡에 저장을 하면 내부적으로 알아서 큰 파일을 128MB로 나눠서 블록으로 저장

렉 단위의 서버 안에 디스크가 여러개 있다고 치면, 각각 다른 서버에 블록 레플리케이션이 분산 저장되는 형태의 아키텍쳐로 구성

서버 하나가 장애가 났다면 (예를 들면 nic 카드가 죽었다. 서버는 살아있지만 네트워크 통신이 안된다.) 네임노드는 주기적으로 heart beat 통신을 하게 되어있는데, 슬레이브 -> 마스터노드 3초마다 통신, 일정시간동안 헐빗 못보내게 되면  해당 노드를 장애가 났다고 판단. 

네임노드는 전체 클러스터의 저장돼 잇는 모든 파일의 블록들이 어디 디스크에 저장되어있는지 알고있는 상태인데, 죽었다고 판단하면 둘 중 한놈한테 블록 하나 더 복사하라는 커멘트 명령어를 전송, 결국 3 카피를 유지하게 됌.

장애 서버가 정상적으로 되면 네임노드한테 다시 리포트를 보내게 되는데, 그럼 블록이 4개가 된다. 네임노드는 스스로 캐치해서 데이터 노드끼리 알아서 복사 하고 제대로 됐는지 리포트를 확인하게 된다.

하둡에서 데이터 유실이 날 수 있는 케이스는 : 세 개의 데이터 노드가 동시에 장애가 나면 유실할 수 있는 가능성은 있으나 굉장히 적다.



**마스터 노드 고장났을 때의 대비책**

1.0 에는 대응책이 없고,

2.0 에는 stanby name node 마스터 서버의 이중화



**블록 지역성** : 큰 데이터가 저장되어 있어도 일차적으로 굉장히 빨리 연산을 할 수 있는 중요한요소

맵리듀스 프로그램을 실행하게 되면 실제 연산을 할때 데이터 측면에서 어떻게 실행되냐면, 슬레이브 노드 하나에 데이터 노드 + 테ㅔ스크 트레커를 띄운다고 햇는데 실제 데이터를 처리하는 데몬은 테스크 트레커에서 처리하게 됨. 하둡의 마스터는 실제 연산을 처리 해야되는(실제 데이터를 갖고있는) 파일의 블록을 가지고 있는 서버한테 할당을 한다. 그러면 테스크 트레커는 복사나 이런 과정 없이 일단 자기 자신이 가지고 있는 로컬 데이터를 읽어서 처리한다. 이것이 data locality,, 하둡은 이 데이터 로컬리티를 보장할 수 있도록 동작 하게 되어있다. ( 데이터가 있는 곳에 가서 연산을 먼저 수행한다. )

그런데 해당 노드가 다른 연산으로 굉장히 바쁜 상태라면 하둡이 다른 노드한테 얘를 읽어서 처리하라고 어싸인 할수도 있는데 기본적으로 또 같은 렉에 있는 노드에 할당을 하는 로컬리티를 보장하게 되어있다.

마스터 노드 스펙 : 하둡의 데몬은 네임스페이스라는 메타정보들을 네임노드의 메모리에 저장해서 관리를 하게 되어있다. 네임노드느 rpc라는 프로토콜로 통신을 하게 되는데 클러스터가 커질수록 스펙이 올라가야 한다.



예전 Message Passing Interface 같은 병렬 처리를 하는 알고리즘을 구현하려면 관리를 개발자가 해야되는데, 그런것들을 하둡이 플랫폼화 시켜놨다.



블록캐싱 : 하둡에서 자주 쓰는 것들은 블록 캐싱 방식을 사용해서 데이터 노드가 가지고 있는 메ㄹ모리에 캐쉬를 등록 해놓을 수 있다.



파일시스템 이미지 관리를 fsimage라는 걸로 네임노드가 어떤 디스크에 저장을 하게 되어있다. fsimage가 손상되면 하드웨어 데이터 다 날라갈 수도 있다.

fsimage는 말그대로 스냅샷이고 이후 변경되는 것은 에딧 로그에 남겨짐



secondary NN 보조 네임노드

네임노드를 이중화 하는 것처럼 보이지만 그 역할은 아니고,  이미지 파일이 네임노드 디스크 어딘가에 남아있는데, 네임 노드의 메모리엔느 항상 최신상태의 이미지가 남아있게 된다.

네임노드가 처음에 데몬을 구동시키면 fs이미지를 쭉 읽어서 메모리에다가 그 스냅샷을 구성한다. 전체 하둡 클러스터에 있는 파일정보 메타 정보들을 구성을 하고 그 다음에 에딧 로그를 읽어가면서 변경된 내용을 메모리에 다 반영,, 그게 다 끝나면 네임노드 구성이 다 끝나고 서비스가 시작된다. 근데 그 이후로 운영 중에 파일 시스템의 변경이 일어났다면, 전부 edit log에 남게된다. 그걸 fs이미지에 병합을 해줘야 되는데 병합을 해줄 때 네임 노드에서 직접 하지 않고 세컨더리로 두 파일을 보내고 snn에서 merge를 한 다음에 실제로 fs 이미지를 바꿔치기 해주는 작업을 주기적으로 함.

즉, fs이미지를 최근 네임노드의 메모리의 상태 정보와 거의 유사한 상태 그리고 에딧로그의 양이 최소한으로 줄어들 수 있도록 머지하느 작업

세컨더리노드 장애가 나면 큰 문제가 생기지는 않는다. 그럼 언제문제가 생기냐면 에딧 로그가 무한히 커지는 문제가 생김. 그래서 서비스를 restart 하게 되면 에딧로그 파일이 너무 크면 읽지못하고 out of memory exception이 발생할 수 있다.



데이터노드의 역할

마스터 노드한테 내가 가지고 있는 데이터를 주기적으로 계속 리포트를 하는 형태, 실제로 데이터는 데이터 노드 로컬파일 시스템에 저장되어 있다. 네임 노드한테 주기적으로 블록 리포트를 해서 블록을 안전한 상태로 항상 유지하는 역할



HDFS 쓰기 연산 처리 메커니즘

리플 3 복사할때 클라이언트 파일을 하둡에 저장할거야 네임노드에 요청을 하게 되면 네임노드가 어이다 복사 한다는 것을 전달,, 즉 클라이언트는 서버 1대랑만 통신한다. 또 리플리케이션 3을 하는 과정은 데이터 노드들 끼리 알아서 하게돼있다.



Rack Awareness 설정

렉단위로 장애가 날 수 있기 때문에 하둡에 데이터를 적재를 할때 서로 다른 렉이 복사가 될 수 있도록 설정하게 되어있음. 데이터가 유실되지 않도록 대응을 하는 부분



HDFS 세이프 모드

데이터 노드를 수정할 수 없는 상태가 됌

레플리케이션이 3이여야 하는데 없는 블록을 미싱블럭이라고 함. 미싱블럭이 일정 퍼센트 이상 발생하는 경우 세이프 모드로 들어감 혹은 클러스터를 재구동 했을 때도 처음에 네임스페이스에 정보를 다 구성하기 전까지는 세이프 모드로 동작하게 됌.

커럽트 블록

파일 자체가 깨진 경우, 유실이 발생한 경우임.

HDFS 휴지통

데이터를 지울 때 물리적으로 바로 지워버리는게 아니라 네임노드 메모리에서만 일단 삭제를 하고 실제로는 트레쉬로 보내게 되어있다. 언제까지 갖고 있을 거냐를 설정할 수 있음.



web hdfs rest API

HDFS 암호화



**하둡 2.0 설명**

![image-20211129232514988](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20211129232514988.png)

1.0과 가장 큰 차이는 마스터 서버의 장애를 해결하기 위한 부분이 가장 큰 변화 포인트,

액티브 네임노드/ 세컨더리 네임노드 는 똑같음 + 스탠바이 네임노드

데이터 노드가 블락 리포트를 한다던지 네임노드와 통신하는 것을 항상 양쪽으로 통신하게 구성이 되어있고, 스탠바이는 정보는 가지고 있지만 평소에는 동작하지 않다가 액티브가 다운됐을 때 액티브로 전환이 되게 되고 네임노드가 이중화 구성

스탠바이가 네임노드로 페일오버 되는 과정에 하둡 네임노드에 들어가 있는 메타정보가 얼마나 크냐에 따라 약간의 다운타임이 발생할 수 있다.

또 네임노드 2.0 기준으로 제일 중요한 포인트는 shared edit logs인데, 1.0에서는 네임노드가 하나일때 fs이미지 파일이 네임노드 로컬 디스크 어딘가에 저장이 되게 구성이 되어있다. 이떄 fs이미지 날라가면 하둡이 망하는거다. 그래서 보통 1.0에서도 fs이미지를 2,3개로 설정할 수 있어서 복수의 물리적인 디스크에 동시에 쓰게된다. 디스크 하나가 날라간다해서 유실되지는 않다. 그렇다면 네임노드가 2개(엑티브,스탠바이)가 되면 그 에딧 로그를 어디다가 관리를 해야하냐?  --> 네임노드 고가용성과 연결 ;; 저널 노드라는 데몬이 별도로 뜬다. 하둡은 저널노드를 여러대 띄울 수 있는 부분을 자체적으로 2.0에서 가지고 있다. 그래서 액티브와 스탠바이가 저널노드에 저장되는 fs이미지와 에딧 로그를 서로 공유할 수 있는 쉐어드 방식이 있는데, 실제로 저널노드도 그냥 물리적인 서버인데 그러면 이미지 파일을 어디다 저장을 할거냐는 이슈가 생겨난다. --> 공유방식1, nfs 파일 시스템에 공유하는 방식, 한계점 : 네트워크 장애가 발생하는 경우





HDFS Federation

네임노드 이중화와 다른개념임

네임로드의 관리를 하다보면 하드 클러스터를 운영하다보면 하나의 물리적인 서버 네임노드 안에 올릴수있는 메모리를 넘어서는 양을 운영해야하는 경우가 발생, 이런경우 네임노드 자체를 여러대로 운영해야 하는 상황이 발생할 수가 있는데 그게 페더레이션이다. 네임노드를 여러개  띄워서 블록 풀을 구성하는것을 지원함. 네임노드도 스케일 아웃 확장을 해나가는 상태를 2.0에서는 지원하고 있다. (국내에서는 적용하는걸 본적이 없다..)



**새로운 플랫폼 주키퍼가 등장**

액티브 네임노드에 대한 선출, 코디네이션 역할

주키퍼는 기본적으로 5, 3대 이상의 서버를 하나의 앙상블로 구성하고 

주키퍼 와치 알고리즘을 통해 마스터 선출, db의 lock 처럼



# 맵리듀스

맵리듀스는 태스크 간의 상호 의존성이 없는 비공유 아키텍처이기 때문에, 실패에 대해 크게 고민하지 않아도 된다.(실패한 테스크를 자동으로 감지하여 장애가 없는 머신에 다시 배치)



# YARN





# 주키퍼





하둡 3.0 차이